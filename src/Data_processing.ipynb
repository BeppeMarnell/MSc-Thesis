{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b090b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from pathlib import Path\n",
    "from rasterio.plot import show\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "import shutil\n",
    "import random\n",
    "from random import randint\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Optional: set the default dpi of the plots\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a tif image. In this case we only show the second band. \n",
    "fp = \"../images/australia_601.tif\"\n",
    "img = rasterio.open(fp)\n",
    "show((img, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read the image and flatten it to a 1D array.\n",
    "array = img.read()[4].flatten()\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Problems with the data:\n",
    "\n",
    "- The data from the tif files present a lot of NaN values. This is due to clouds and \n",
    "satellite data not always being clear. We need to clean them before we can use them.\n",
    "\n",
    "- Each image is not 200x200. We need to crop the image to 200x200 to fit into the model.\n",
    "\n",
    "- The data is not normalised. We need to normalise the data before we can use it.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def count_nan(image):\n",
    "    \"\"\"\n",
    "    Count the amount of NaN values in the image.\n",
    "    \"\"\"\n",
    "    count_nans = np.empty(image.shape[0], dtype=\"float32\")\n",
    "    \n",
    "    for idx in range(0, image.shape[0]):\n",
    "        array = image[idx].flatten()\n",
    "        count = np.isnan(array).sum()\n",
    "        count_nans[idx] = count\n",
    "        \n",
    "    return count_nans\n",
    "    \n",
    "def show_uniques(image):\n",
    "    \"\"\"\n",
    "    Show the unique values in the image.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(image, return_counts=True)\n",
    "    return unique, counts\n",
    "\n",
    "def split_image(image, img_size):\n",
    "    \"\"\"\n",
    "    Split the image into 200x200 images.\n",
    "    \"\"\"\n",
    "    \n",
    "    old_imgshape_y = int(image.shape[2]/2)\n",
    "    half_img = int(img_size/2)\n",
    "    old_imgshape_x = int(image.shape[1]/2)\n",
    "    array_img_1 = image[:, (old_imgshape_x - half_img):(old_imgshape_x + half_img), (old_imgshape_y - half_img):(old_imgshape_y + half_img)]\n",
    "    return array_img_1\n",
    "\n",
    "def get_label(image):\n",
    "    \"\"\"\n",
    "    Get the truth label from the tif image. It is the first layer of the tif image.\n",
    "    \n",
    "    We format the label to be 0 for no fire and 1 for fire.\n",
    "    We pack the rest of the features into a single array.\n",
    "    \"\"\"\n",
    "    \n",
    "    label = image[0]\n",
    "    idxs_fire = np.where(label >= 1)\n",
    "\n",
    "    label[:] = 0\n",
    "    label[idxs_fire] = 1\n",
    "\n",
    "    # Remove the first layer\n",
    "    array_img = np.delete(image, 0, 0)\n",
    "    \n",
    "    return array_img, label\n",
    "\n",
    "def get_label_two_classes(image):\n",
    "    \"\"\"\n",
    "    Get the truth label from the tif image. It is the first layer of the tif image.\n",
    "    \n",
    "    We use this method to count the unique fire instances in the image, as there are \n",
    "    multiple instances of fire in the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    label = image[0]\n",
    "    uniqlo, cunts = show_uniques(label)\n",
    "    \n",
    "    counter = 0\n",
    "    for idx in range(0,len(uniqlo)):\n",
    "        if(uniqlo[idx] > 400):\n",
    "            counter = counter + cunts[idx] \n",
    "    \n",
    "    label = 0\n",
    "    if counter > 15:\n",
    "        label = 1\n",
    "\n",
    "    # Remove the first layer\n",
    "    array_img = np.delete(image, 0, 0)\n",
    "    return array_img, label\n",
    "\n",
    "def convert_landcover(image):\n",
    "    \"\"\"\n",
    "    Method to covert the landcover feature into 0s and 1s.\n",
    "    This is mostly based on lands that can be flamable and not.\n",
    "    \"\"\"\n",
    "    \n",
    "    last_idx = image.shape[0] -1\n",
    "    tmp_arr = image[last_idx]\n",
    "    \n",
    "    tmp_arr[tmp_arr == 1.] = 1.\n",
    "    tmp_arr[tmp_arr == 2.] = 1.\n",
    "    tmp_arr[tmp_arr == 3.] = 1.\n",
    "    tmp_arr[tmp_arr == 4.] = 1.\n",
    "    tmp_arr[tmp_arr == 5.] = 1.\n",
    "    tmp_arr[tmp_arr == 6.] = 1.\n",
    "    tmp_arr[tmp_arr == 7.] = 1.\n",
    "    tmp_arr[tmp_arr == 8.] = 1.\n",
    "    tmp_arr[tmp_arr == 9.] = 1.\n",
    "    tmp_arr[tmp_arr == 10.] = 1.\n",
    "    tmp_arr[tmp_arr == 11.] = 0.\n",
    "    tmp_arr[tmp_arr == 12.] = 0.\n",
    "    tmp_arr[tmp_arr == 13.] = 0.\n",
    "    tmp_arr[tmp_arr == 14.] = 1.\n",
    "    tmp_arr[tmp_arr == 15.] = 0.\n",
    "    tmp_arr[tmp_arr == 16.] = 0.\n",
    "    tmp_arr[tmp_arr == 17.] = 0.\n",
    "    return image\n",
    "\n",
    "def remove_nan_zeros(image, idx, value=0):\n",
    "    image[idx] = np.nan_to_num(image[idx], value)\n",
    "    return image\n",
    "\n",
    "def remove_nan_mean(image, idx):\n",
    "    mean = np.nanmean(image[idx])\n",
    "    if math.isnan(mean):\n",
    "        mean = 0\n",
    "    image[idx] = np.nan_to_num(image[idx], nan=mean)\n",
    "    return image\n",
    "\n",
    "def clean_features(image):\n",
    "    \"\"\"\n",
    "    Method to clean the features of the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    idxs = range(2, image.shape[0] - 2)\n",
    "    \n",
    "    # clean the fires\n",
    "    image = remove_nan_zeros(image,0)\n",
    "    # clean the dem\n",
    "    image = remove_nan_zeros(image,1)\n",
    "    \n",
    "    for idx in idxs:\n",
    "        image = remove_nan_mean(image, idx)\n",
    "    \n",
    "    # clean the history of fires\n",
    "    image = remove_nan_zeros(image, (image.shape[0] - 2))\n",
    "    \n",
    "    # clean last feature\n",
    "    remove_nan_zeros(image , image.shape[0]-1, value=17)\n",
    "    \n",
    "    idfss = range(0, image.shape[0])\n",
    "    for idf in idfss:\n",
    "        check_fornan(image[idf])\n",
    "    \n",
    "    return image\n",
    "\n",
    "def check_fornan(image):\n",
    "    flat_image = np.asarray(image).flatten()\n",
    "    \n",
    "    array = np.isnan(flat_image)\n",
    "    sum_s = np.sum(array)\n",
    "    if sum_s > 0:\n",
    "        print(\"with null\")\n",
    "        \n",
    "def normalise(image):\n",
    "    img_min = np.min(image)\n",
    "    img_max = np.max(image)\n",
    "    image_tmp = (image - img_min) / (img_max - img_min)\n",
    "    return image_tmp\n",
    "\n",
    "def normalise_features(dataset, features):\n",
    "    for feat in range(0, features):\n",
    "        dataset[:, feat, :, :] =  normalise(dataset[:, feat, :, :])\n",
    "        \n",
    "def check_forones(label):\n",
    "    if np.sum(label) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def cut_array_to(arr, des_length):\n",
    "    \"\"\"\n",
    "    Cut the array to the desired length.\n",
    "    \"\"\"\n",
    "    to_cut = range(des_length, (arr.shape[0] - des_length) + des_length)\n",
    "    print(to_cut)\n",
    "    \n",
    "    return np.delete(arr, to_cut, axis=0)\n",
    "\n",
    "def check_size(img):\n",
    "    if img.shape[1] <200 or img.shape[2] <200:\n",
    "        print('image is smaller')\n",
    "\n",
    "        \n",
    "def get_tif_in_path(path):\n",
    "    dataset_files = []\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        dataset_files.extend(filenames)\n",
    "        break\n",
    "\n",
    "    dataset_files = [file for file in dataset_files if '.tif' in file ]\n",
    "    \n",
    "    return dataset_files\n",
    "\n",
    "def split_and_save_datasets(train_size=900, val_size=150, path='', img_features=20):\n",
    "    \"\"\"\n",
    "    Method to split the dataset into training and validation datasets.\n",
    "    \n",
    "    Understand that this method is not the most efficient, but it is a good starting point.\n",
    "    \n",
    "    train_size: the size of the training dataset\n",
    "    val_size: the size of the validation dataset\n",
    "    path: the path where the dataset is located\n",
    "    img_features: the amount of features in the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # We set the desired image size\n",
    "    IMG_SIZE = int(200)\n",
    "    base_path = 'your-path/' + path\n",
    "    \n",
    "    datasets = {'dataset':train_size, 'dataset_val': train_size + val_size}\n",
    "    \n",
    "    # Create the numpy arrays to store the data\n",
    "    arr_imgs = np.empty([train_size + val_size, img_features, IMG_SIZE, IMG_SIZE], dtype=\"float32\")\n",
    "    arr_label = np.empty([train_size + val_size, IMG_SIZE, IMG_SIZE], dtype=\"float32\")\n",
    "    \n",
    "    count_nans = np.empty([train_size + val_size, img_features + 1], dtype=\"float32\")\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    for datas_key in datasets:\n",
    "        print('Processing ' + datas_key)\n",
    "        subpath = base_path + '/' + datas_key\n",
    "        \n",
    "        # gather the files\n",
    "        dataset_files = get_tif_in_path(subpath)\n",
    "        \n",
    "        # count the countries\n",
    "        count_countries = {'africa':0, 'australia':0, 'eurasia':0, 'south_america':0, 'us':0, 'europe':0, 'asia':0}\n",
    "        \n",
    "        for file in tqdm(dataset_files):\n",
    "            if idx == datasets[datas_key]:\n",
    "                break\n",
    "\n",
    "            path = subpath + '/'+ file\n",
    "\n",
    "            # read the image as an array\n",
    "            tmp_img = rasterio.open(path)\n",
    "            array = tmp_img.read() #np.nan_to_num(tmp_img.read())\n",
    "\n",
    "            split_img = split_image(array, IMG_SIZE)\n",
    "            \n",
    "            # count the amount of nans\n",
    "            count_nans[idx] = count_nan(split_img)\n",
    "            \n",
    "            # remove nan from fatures\n",
    "            cleaned_img = clean_features(split_img)\n",
    "            cleaned_img, label = get_label(cleaned_img)\n",
    "            \n",
    "            if cleaned_img.shape[0] < 16:\n",
    "                continue\n",
    "\n",
    "            # if there are not ones, skip\n",
    "            if not check_forones(label):\n",
    "                continue\n",
    "                \n",
    "            # count the instances\n",
    "            for key in count_countries:\n",
    "                if key in file:\n",
    "                    count_countries[key] = count_countries[key] + 1\n",
    "                    break\n",
    "\n",
    "            arr_imgs[idx,] = cleaned_img\n",
    "            arr_label[idx] = label\n",
    "            idx += 1\n",
    "            \n",
    "        print(datas_key, count_countries)\n",
    "\n",
    "    # cut the array to only with ones labels\n",
    "    arr_imgs = cut_array_to(arr_imgs, idx)\n",
    "    arr_label = cut_array_to(arr_label, idx)\n",
    "    count_nans = cut_array_to(count_nans, idx)\n",
    "    \n",
    "    print('Saving the training dataset ...')\n",
    "    np.savez_compressed(base_path + '/dataset/Xdataset.npy', arr_imgs[0:train_size,])\n",
    "    np.savez_compressed(base_path + '/dataset/Ydataset.npy', arr_label[0:train_size,])\n",
    "\n",
    "    print('Saving the validation dataset ...')\n",
    "    np.savez_compressed(base_path + '/dataset_val/Xdataset_val.npy', arr_imgs[train_size:,])\n",
    "    np.savez_compressed(base_path + '/dataset_val/Ydataset_val.npy', arr_label[train_size:,])\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_save_datasets(path='', val_size=150, img_features=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
