{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2mp4_RUbn6R",
        "outputId": "97fdb143-3d4a-4c76-9611-b09402a94e7d"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Hi, I am Giuseppe Marinelli and this is the python code for my master thesis in which I tried to\n",
        "assess the risk of wildfires with a ML model and remote sensing data.\n",
        "Altough this implementation may not be the best, I have spent a lot of hours trying to figure things out.\n",
        "Hopefully, the code that I share here, may one day help someone.\n",
        "\n",
        "You can use my code without problems (maybe a little mention ?).\n",
        "\n",
        "For further info you can contact me: marinellibeppe@gmail.com\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model, layers\n",
        "from keras import backend as K\n",
        "from keras.backend import clear_session\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "# Here I am using google colab, so I need to mount my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a reference to the database path\n",
        "drive_path = '/data_path/'\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QHm2O86dlen"
      },
      "source": [
        "### Load train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDdROQIWcWi2"
      },
      "outputs": [],
      "source": [
        "def load_dataset(batch=32, buffer=200):\n",
        "    \"\"\"\n",
        "    Method to load the dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    # load the train dataset\n",
        "    X_train = np.load(drive_path + 'Xdataset.npy.npz')['arr_0']\n",
        "    X_train = X_train.transpose(0,2,3,1)\n",
        "    Y_train = np.load(drive_path + 'Ydataset.npy.npz')['arr_0']\n",
        "\n",
        "    Y_train = np.reshape(Y_train, (Y_train.shape[0],200,200,1))\n",
        "    \n",
        "    #Â load the eval dataset\n",
        "    X_val = np.load(drive_path + 'Xdataset_val.npy.npz')['arr_0']\n",
        "    X_val = X_val.transpose(0,2,3,1)\n",
        "    Y_val = np.load(drive_path + 'Ydataset_val.npy.npz')['arr_0']\n",
        "\n",
        "    Y_val = np.reshape(Y_val, (Y_val.shape[0], 200,200,1))\n",
        "    \n",
        "    # load datasets \n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "\n",
        "    train_dataset = train_dataset.shuffle(buffer).batch(batch)\n",
        "    val_dataset = val_dataset.batch(batch)\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def load_rnd_dataset(size=200, batch=32, buffer=100):\n",
        "    \"\"\"\n",
        "    Method to load a random syntetic dataset for testing purposes\n",
        "    \"\"\"\n",
        "    X_train = np.random.rand(20, size, size, 19)\n",
        "    Y_train = np.ones((20, size, size))\n",
        "\n",
        "    X_val = np.random.rand(4, size, size, 19)\n",
        "    Y_val = np.ones((4, size, size))\n",
        "    \n",
        "    # load datasets \n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
        "\n",
        "    train_dataset = train_dataset.shuffle(buffer).batch(batch)\n",
        "    val_dataset = val_dataset.batch(batch)\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "# train_dataset, val_dataset = load_rnd_dataset() # Syntetic dataset\n",
        "train_dataset, val_dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvWyR7icpmyR",
        "outputId": "27cc211c-f2e7-4960-a7fa-2e45bc231412"
      },
      "outputs": [],
      "source": [
        "# Let's visualize some of the images\n",
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "\n",
        "    image = image_batch[i].numpy()\n",
        "\n",
        "    plt.imshow(image[:,:,0])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z79T2U1lpXyJ"
      },
      "source": [
        "### Define loss function and accuracies "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28CdDMwbpXG7"
      },
      "outputs": [],
      "source": [
        "def ratio_fires(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method to calculate the ratio between the found fires and the total fires.\n",
        "    \"\"\"\n",
        "    y_truef = tf.where(y_true > 0.5, 1., 0.)\n",
        "    y_predf = tf.where(y_pred > 0.5, 1., 0.)\n",
        "\n",
        "    masked = y_truef * y_predf\n",
        "    return tf.reduce_sum(masked) / tf.reduce_sum(y_truef)\n",
        "\n",
        "\n",
        "def ratio_no_fires(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method to calculate the ratio between the found no fires and the total no fires.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here we flip both arrays, 0 are the ones > 0.5\n",
        "    y_truef = tf.where(y_true > 0.5, 0., 1.)\n",
        "    y_predf = tf.where(y_pred > 0.5, 0., 1.)\n",
        "\n",
        "    intersection = y_truef * y_predf\n",
        "    return tf.reduce_sum(intersection) / tf.reduce_sum(y_truef)\n",
        "\n",
        "\n",
        "# Set the smooth value to avoid division by zero\n",
        "smooth = smooth2 = 1\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method to calculate the dice coefficient.\n",
        "    \"\"\"\n",
        "\n",
        "    y_truef = tf.reshape(y_true, [-1])\n",
        "    y_predf = tf.reshape(y_pred, [-1])\n",
        "\n",
        "    intersection = tf.reduce_sum(y_truef * y_predf)\n",
        "    dice = (2.0 * intersection) / \\\n",
        "        (tf.reduce_sum(y_truef) + tf.reduce_sum(y_predf))\n",
        "\n",
        "    return dice\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Method to calculate the dice loss.\n",
        "    This is the loss function that we will use to train the model.\n",
        "    \"\"\"\n",
        "    return 1 - dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def iou_coef(y_true, y_pred):\n",
        "    intersection = tf.reduce_sum(tf.abs(y_true * y_pred))\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    iou = tf.reduce_mean((intersection + smooth2) / (union + smooth2))\n",
        "    return iou\n",
        "\n",
        "\n",
        "class CutDownBS(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Method to keep track of the output of the model and \n",
        "    stop the training if the output does not change.\n",
        "\n",
        "    You can remove this callback if you want.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=4):\n",
        "        self.tracking = []\n",
        "        self.patience = patience\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.tracking.append(np.around(logs['ratio_fires'], decimals=4))\n",
        "\n",
        "        np_arr = np.asarray(self.tracking)\n",
        "\n",
        "        idx1 = 0\n",
        "        idx2 = idx1 + 1\n",
        "\n",
        "        for _ in range(0, len(np_arr)):\n",
        "            if idx2 == len(np_arr):\n",
        "                break\n",
        "\n",
        "            if np_arr[idx1] == np_arr[idx2]:\n",
        "                idx2 = idx2 + 1\n",
        "\n",
        "                if ((idx2 - idx1) >= self.patience):\n",
        "                    self.model.stop_training = True\n",
        "                    print('Stopping training ...')\n",
        "            else:\n",
        "                idx1 = idx2\n",
        "                idx2 = idx2 + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JNXn5Iyd080"
      },
      "source": [
        "### Train the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PftlHYJmdqKl",
        "outputId": "12d3e7c3-39f4-4da8-feff-e0f3819bd83c"
      },
      "outputs": [],
      "source": [
        "clear_session()\n",
        "\n",
        "def cnn(input_size=(200, 200, 20), dropout=0.1, lay_deep=2):\n",
        "    inputs = layers.Input(input_size)\n",
        "\n",
        "    x = layers.Conv2D(40, (7,7), padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(60, (5,5), padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    chann_deep = x.shape[-1]\n",
        "    for l_d in range(0,lay_deep):\n",
        "        chann_deep  += 10\n",
        "        x = layers.Conv2D(chann_deep, (3,3), padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('tanh')(x)\n",
        "\n",
        "    x = layers.Conv2D(30, (3,3), padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Conv2D(1, (3,3), padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    model_output = layers.Conv2D(1, (1,1), activation='sigmoid', dtype=tf.float32)(x)\n",
        "    \n",
        "    return Model(inputs=[inputs], outputs=[model_output], name=\"model_cnn\")\n",
        "\n",
        "model_cnn = cnn(dropout=0.3, lay_deep=6)\n",
        "\n",
        "# model_cnn.summary()\n",
        "\n",
        "def step_decay(epoch):\n",
        "   initial_lrate = 0.001\n",
        "   drop = 0.2\n",
        "   epochs_drop = 10.0\n",
        "   lrate = initial_lrate * math.pow(drop,  math.floor((1+epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "initial_learning_rate = 0.0008\n",
        "\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.2\n",
        "    return initial_learning_rate * math.exp(-k*epoch)\n",
        "\n",
        "lrate_call = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)\n",
        "\n",
        "model_cnn.compile(optimizer = tf.keras.optimizers.Adam(), \n",
        "              loss = dice_coef_loss, \n",
        "              metrics = [ratio_fires, ratio_no_fires, iou_coef, dice_coef])\n",
        "\n",
        "\n",
        "logdir = drive_path\n",
        "file_name = 'modelz_try1'\n",
        "\n",
        "callbacks = [lrate_call, keras.callbacks.ModelCheckpoint(logdir + file_name + '.hdf5', save_best_only=True,\n",
        "                                             verbose=1, monitor='val_dice_coef', mode='max'), CutDownBS()]\n",
        "\n",
        "# train the model\n",
        "hist_model = model_cnn.fit(train_dataset, \n",
        "                       validation_data = val_dataset, \n",
        "                       epochs = 30,\n",
        "                       callbacks=callbacks)\n",
        "\n",
        "with open(logdir + file_name + '.pickle', 'wb') as handle:\n",
        "            pickle.dump(hist_model.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "5X3LyvOTzR9k",
        "outputId": "ed098f53-4e39-461b-88c5-b72687c0798d"
      },
      "outputs": [],
      "source": [
        "arr = [hist_model.history['loss'], hist_model.history['val_loss'], hist_model.history['ratio_fires'], hist_model.history['val_ratio_fires'], ]\n",
        "labels = ['train loss', 'val loss', 'train acc', 'val acc']\n",
        "\n",
        "count=1\n",
        "for col in range(0,2):\n",
        "    plt.subplot(1, 2, count)\n",
        "\n",
        "    idx = col * 2 \n",
        "    \n",
        "    plt.plot(arr[idx], label=labels[idx])\n",
        "    plt.plot(arr[idx + 1], label=labels[idx + 1])\n",
        "\n",
        "    count += 1\n",
        "    plt.legend()\n",
        "\n",
        "plt.gcf().set_size_inches(18, 5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRwMxYqPaeSY"
      },
      "source": [
        "### Train U_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE40sSAyahg_"
      },
      "outputs": [],
      "source": [
        "def unet(input_size=(200, 200, 20)):\n",
        "    inputs = layers.Input(input_size)\n",
        "    \n",
        "    conv1 = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    bn1 = layers.Activation('relu')(conv1)\n",
        "    conv1 = layers.Conv2D(64, (3, 3), padding='same')(bn1)\n",
        "    bn1 = layers.BatchNormalization(axis=3)(conv1)\n",
        "    bn1 = layers.Activation('relu')(bn1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(bn1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, (3, 3), padding='same')(pool1)\n",
        "    bn2 = layers.Activation('relu')(conv2)\n",
        "    conv2 = layers.Conv2D(128, (3, 3), padding='same')(bn2)\n",
        "    bn2 = layers.BatchNormalization(axis=3)(conv2)\n",
        "    bn2 = layers.Activation('relu')(bn2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(bn2)\n",
        "\n",
        "    conv3 = layers.Conv2D(256, (3, 3), padding='same')(pool2)\n",
        "    bn3 = layers.Activation('relu')(conv3)\n",
        "    conv3 = layers.Conv2D(256, (3, 3), padding='same')(bn3)\n",
        "    bn3 = layers.BatchNormalization(axis=3)(conv3)\n",
        "    bn3 = layers.Activation('relu')(bn3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(bn3)\n",
        "\n",
        "    conv4 = layers.Conv2D(512, (3, 3), padding='same')(pool3)\n",
        "    bn4 = layers.Activation('relu')(conv4)\n",
        "    conv4 = layers.Conv2D(512, (3, 3), padding='same')(bn4)\n",
        "    bn4 = layers.BatchNormalization(axis=3)(conv4)\n",
        "    bn4 = layers.Activation('relu')(bn4)\n",
        "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(bn4)\n",
        "\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), padding='same')(pool4)\n",
        "    bn5 = layers.Activation('relu')(conv5)\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), padding='same')(bn5)\n",
        "    bn5 = layers.BatchNormalization(axis=3)(conv5)\n",
        "    bn5 = layers.Activation('relu')(bn5)\n",
        "\n",
        "    up6 = layers.concatenate([layers.Conv2DTranspose(512, (3, 3), strides=(2, 2))(bn5), conv4], axis=3)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), padding='same')(up6)\n",
        "    bn6 = layers.Activation('relu')(conv6)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), padding='same')(bn6)\n",
        "    bn6 = layers.BatchNormalization(axis=3)(conv6)\n",
        "    bn6 = layers.Activation('relu')(bn6)\n",
        "\n",
        "    up7 = layers.concatenate([layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bn6), conv3], axis=3)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), padding='same')(up7)\n",
        "    bn7 = layers.Activation('relu')(conv7)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), padding='same')(bn7)\n",
        "    bn7 = layers.BatchNormalization(axis=3)(conv7)\n",
        "    bn7 = layers.Activation('relu')(bn7)\n",
        "\n",
        "    up8 = layers.concatenate([layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(bn7), conv2], axis=3)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), padding='same')(up8)\n",
        "    bn8 = layers.Activation('relu')(conv8)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), padding='same')(bn8)\n",
        "    bn8 = layers.BatchNormalization(axis=3)(conv8)\n",
        "    bn8 = layers.Activation('relu')(bn8)\n",
        "\n",
        "    up9 = layers.concatenate([layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(bn8), conv1], axis=3)\n",
        "    conv9 = layers.Conv2D(64, (3, 3), padding='same')(up9)\n",
        "    bn9 = layers.Activation('relu')(conv9)\n",
        "    conv9 = layers.Conv2D(64, (3, 3), padding='same')(bn9)\n",
        "    bn9 = layers.BatchNormalization(axis=3)(conv9)\n",
        "    bn9 = layers.Activation('relu')(bn9)\n",
        "\n",
        "    conv10 = layers.Conv2D(1, (1, 1), activation='sigmoid', dtype=tf.float32)(bn9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "model_u = unet()\n",
        "model_u.summary()\n",
        "\n",
        "model_u.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.6), \n",
        "                loss = dice_coef_loss, \n",
        "                metrics = [ratio_fires, ratio_no_fires, iou_coef, dice_coef])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shwISvV0azBJ"
      },
      "outputs": [],
      "source": [
        "callbacks_u = [keras.callbacks.ModelCheckpoint(drive_path + 'model_u_thesis.hdf5', save_best_only=True,\n",
        "                                             verbose=1, monitor='val_dice_coef', mode='max'), CutDownBS()]\n",
        "\n",
        "hist_model_u = model_u.fit(train_dataset, \n",
        "                           validation_data = val_dataset, \n",
        "                           epochs = 30,\n",
        "                           use_multiprocessing=True, \n",
        "                           workers=6, \n",
        "                           callbacks=callbacks_u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VpL5uVoX7IR"
      },
      "source": [
        "## Train Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZkxHM-TX9t3"
      },
      "outputs": [],
      "source": [
        "def autoenc(input_size=(200, 200, 20)):\n",
        "    inputs = layers.Input(input_size)\n",
        "\n",
        "    # encoder\n",
        "    conv1_1 = layers.Conv2D(40, (5, 5), activation='tanh', padding='same')(inputs)\n",
        "    pool1 = layers.MaxPooling2D((2, 2), padding='same')(conv1_1)\n",
        "    conv1_2 = layers.Conv2D(20, (3, 3), activation='tanh', padding='same')(pool1)\n",
        "    pool2 = layers.MaxPooling2D((2, 2), padding='same')(conv1_2)\n",
        "    conv1_3 = layers.Conv2D(5, (3, 3), activation='tanh', padding='same')(pool2)\n",
        "    h = layers.MaxPooling2D((2, 2), padding='same')(conv1_3)\n",
        "    \n",
        "    # Decoder\n",
        "    conv2_1 = layers.Conv2D(5, (3, 3), activation='tanh', padding='same')(h)\n",
        "    up1 = layers.UpSampling2D((2, 2))(conv2_1)\n",
        "    conv2_2 = layers.Conv2D(20, (3, 3), activation='tanh', padding='same')(up1)\n",
        "    up2 = layers.UpSampling2D((2, 2))(conv2_2)\n",
        "    conv2_3 = layers.Conv2D(40, (3, 3), activation='tanh', padding='same')(up2)\n",
        "    up3 = layers.UpSampling2D((2, 2))(conv2_3)\n",
        "    r = layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same', dtype=tf.float32)(up3)\n",
        "    \n",
        "    return Model(inputs=[inputs], outputs=[r])\n",
        "\n",
        "model_auto = autoenc()\n",
        "model_auto.summary()\n",
        "\n",
        "model_auto.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.6), \n",
        "                loss = dice_coef_loss, \n",
        "                metrics = [ratio_fires, ratio_no_fires, iou_coef, dice_coef])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WM1H3YAjZ7g"
      },
      "outputs": [],
      "source": [
        "callbacks_u = [keras.callbacks.ModelCheckpoint(drive_path + 'model_auto.hdf5', save_best_only=True,\n",
        "                                             verbose=1, monitor='val_dice_coef', mode='max'), CutDownBS()]\n",
        "\n",
        "hist_model_auto = model_auto.fit(train_dataset, \n",
        "                                validation_data = val_dataset, \n",
        "                                epochs = 30,\n",
        "                                use_multiprocessing=True, \n",
        "                                workers=6, \n",
        "                                callbacks=callbacks_u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "c1G5B8u5kfsc",
        "outputId": "efc53908-eb8e-4b71-f9e7-ad646de91a7a"
      },
      "outputs": [],
      "source": [
        "arr = [hist_model_auto.history['loss'], hist_model_auto.history['val_loss'], hist_model_auto.history['ratio_fires'], hist_model_auto.history['val_ratio_fires'], ]\n",
        "labels = ['train loss', 'val loss', 'train acc', 'val acc']\n",
        "\n",
        "count=1\n",
        "for col in range(0,2):\n",
        "    plt.subplot(1, 2, count)\n",
        "\n",
        "    idx = col * 2 \n",
        "    \n",
        "    plt.plot(arr[idx], label=labels[idx])\n",
        "    plt.plot(arr[idx + 1], label=labels[idx + 1])\n",
        "\n",
        "    count += 1\n",
        "    plt.legend()\n",
        "\n",
        "plt.gcf().set_size_inches(18, 5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHS-2ssKt_Dg"
      },
      "source": [
        "## Test/verify the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MKdo3fpuIGO"
      },
      "outputs": [],
      "source": [
        "# Load model from saved weights\n",
        "model_cnn.load_weights(drive_path + 'model_cnn_best_6_0.3.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpW6w60_9weN"
      },
      "outputs": [],
      "source": [
        "X_val = np.load(drive_path + 'Xdataset_val.npy.npz')['arr_0']\n",
        "Y_val = np.load(drive_path + 'Ydataset_val.npy.npz')['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "GCGvRpca9_Rs",
        "outputId": "d1bbab06-b47e-47f6-bc94-ac68bb874c51"
      },
      "outputs": [],
      "source": [
        "def compute_label(sample):\n",
        "    #  format the sample\n",
        "    tmp_np = np.moveaxis(sample, 0, 2)\n",
        "\n",
        "    print(tmp_np.shape)\n",
        "\n",
        "    tmp_np = np.reshape(tmp_np, [1, tmp_np.shape[0], tmp_np.shape[1], tmp_np.shape[2]])\n",
        "\n",
        "    test_label = model_cnn.predict(tmp_np)\n",
        "\n",
        "    return test_label[0,:,:,0]\n",
        "\n",
        "idx = randint(0, X_val.shape[0]-1)\n",
        "\n",
        "print(\"index: \", idx)\n",
        "y_true = Y_val[idx]\n",
        "y_pred = X_val[idx]\n",
        "y_predic = compute_label(y_pred)\n",
        "\n",
        "y_pred2 = y_predic.copy()\n",
        "y_pred2[y_pred2 >= 0.6] = 1\n",
        "y_pred2[y_pred2 < 0.6] = 0\n",
        "\n",
        "array = [y_predic, y_pred2, y_true, (y_pred2 + y_true*2), X_val[idx][0]]\n",
        "\n",
        "count = 1\n",
        "\n",
        "for col in array:\n",
        "    plt.subplot(1, 5, count)\n",
        "    \n",
        "    plt.imshow(col)\n",
        "    cbar = plt.colorbar(orientation=\"horizontal\", pad=0.02)\n",
        "    cbar.ax.tick_params(labelsize=13)\n",
        "    count +=1\n",
        "\n",
        "plt.gcf().set_size_inches(25, 25)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_QHm2O86dlen",
        "Z79T2U1lpXyJ",
        "2JNXn5Iyd080",
        "UQoRWtd71fy8",
        "eHS-2ssKt_Dg"
      ],
      "machine_shape": "hm",
      "name": "Wildfires-assessent-keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
